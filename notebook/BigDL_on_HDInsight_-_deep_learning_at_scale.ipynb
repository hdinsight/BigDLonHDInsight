{"nbformat_minor": 2, "cells": [{"source": "# Introduction to BigDL on HDInsight Spark\n## Deep learning at scale\n\n-------------\n\n&nbsp;&nbsp;\n<div style=\"height: 120px;\">\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/96/Microsoft_logo_%282012%29.svg/1280px-Microsoft_logo_%282012%29.svg.png\" style=\"height: 80px; display: inline; \"/> &nbsp;\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c9/Intel-logo.svg/2000px-Intel-logo.svg.png\" style=\"height: 80px; display: inline; \"/>\n</div>\n\n\n\n## Presenters\n* Denny Lee, Principal Program Manager, CosmosDB \n* Tom Drabas, Data Scientist, WDG\n\n## In close cooperation with Intel\n* Sergey Ermolin, Power/Performance Optimization\n* Ding Ding, Software Engineer\n* Jiao Wang, Software Engineer\n* Jason Dai, Senior Principle Engineer and CTO, Big Data Technologies\n* Yiheng, Wang, Software Engineer\n* Xianyan Jia, Software Engineer\n\n## Special thanks to\n* Felix Cheung, Principal Software Engineer\n* Xiaoyong Zhu, Program Manager\n* Alejandro Guerrero Gonzalez, Senior Software Engineer\n\n--------------", "cell_type": "markdown", "metadata": {}}, {"source": "# Setting up the environment\n\n### 1. Clone the Github repository\n\nIn the Github repository you will find all you need to finish this workshop https://github.com/drabastomek/bigdl-fun.git:\n\n1. **data** folder - contains a set of 4 files that can be downloaded from http://yann.lecun.com/exdb/mnist/:\n    1. *train-images-idx3-ubyte* - set of training images in a binary format with a specific schema (we'll get to that)\n    2. *train-labels-idx1-ubyte* - corresponding set of training labels\n    3. *t10k-images-idx3-ubyte* - set of testing (validation) images\n    4. *t10k-labels-idx1-ubyte* - corresponding set of testing (validation) labels\n2. **jars** folder - contains two compiled jars for the BigDL:\n    1. *bigdl-0.2.0-SNAPSHOT-spark-2.0-jar-with-dependencies.jar* - BigDL compiled for Spark 2.0\n    2. *bigdl-0.2.0-SNAPSHOT-spark-2.1-jar-with-dependencies.jar* - BigDL compiled for Spark 2.1\n3. **notebook** folder - contains this notebook \n\n### 2. Upload BigDL jar\n\nGrab the jar from the **jars** folder appropriate for your version of Spark.\n\n1. Go to Azure Dashboard and click on your cluster. Scroll down to the Storage accounts ![Storage options](http://tomdrabas.com/data/BigDL/StorageAccount.png)\n2. Click on the default storage account ![Default storage](http://tomdrabas.com/data/BigDL/DefaultStorageAccount.png)\n3. Go to Blobs ![Blobs](http://tomdrabas.com/data/BigDL/Blobs.png)\n4. Select the default container ![Container](http://tomdrabas.com/data/BigDL/DefaultContainer_obs.png)\n5. Upload the jar appropriate for your version of Spark to the root of the folder ![Upload](http://tomdrabas.com/data/BigDL/Upload_obs.png)\n6. Check if uploaded successfully ![Uploaded](http://tomdrabas.com/data/BigDL/UploadedJar.png)\n\n### 3. Upload the data\n\nSimilarly to uploading the BigDL upload the data from the **data** folder. Upload the data into the `/tmp` folder in your default storage.\n\n-------------", "cell_type": "markdown", "metadata": {}}, {"source": "# Configuring the session", "cell_type": "markdown", "metadata": {}}, {"source": "First, let's configure our session.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{\n    \"jars\": [\"wasb:///bigdl-0.2.0-SNAPSHOT-spark-2.0-jar-with-dependencies.jar\"],\n    \n     \"conf\":{\n         \"spark.executorEnv.DL_ENGINE_TYPE\": \"mklblas\",\n         \"spark.executorEnv.MKL_DISABLE_FAST_MM\": \"1\",\n         \"spark.executorEnv.KMP_BLOCKTIME\": \"0\",\n         \"spark.executorEnv.OMP_WAIT_POLICY\": \"passive\",\n         \"spark.executorEnv.OMP_NUM_THREADS\":\"1\",\n         \"spark.yarn.appMasterEnv.DL_ENGINE_TYPE\": \"mklblas\",\n         \"spark.yarn.appMasterEnv.MKL_DISABLE_FAST_MM\": \"1\",\n         \"spark.yarn.appMasterEnv.KMP_BLOCKTIME\": \"0\",\n         \"spark.yarn.appMasterEnv.OMP_WAIT_POLICY\": \"passive\",\n         \"spark.yarn.appMasterEnv.OMP_NUM_THREADS\": \"1\"\n     }\n}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'jars': [u'wasb:///bigdl-0.2.0-SNAPSHOT-spark-2.0-jar-with-dependencies.jar'], u'kind': 'spark', u'conf': {u'spark.executorEnv.MKL_DISABLE_FAST_MM': u'1', u'spark.yarn.appMasterEnv.DL_ENGINE_TYPE': u'mklblas', u'spark.yarn.appMasterEnv.OMP_WAIT_POLICY': u'passive', u'spark.yarn.appMasterEnv.KMP_BLOCKTIME': u'0', u'spark.executorEnv.DL_ENGINE_TYPE': u'mklblas', u'spark.executorEnv.OMP_WAIT_POLICY': u'passive', u'spark.executorEnv.KMP_BLOCKTIME': u'0', u'spark.yarn.appMasterEnv.MKL_DISABLE_FAST_MM': u'1', u'spark.executorEnv.OMP_NUM_THREADS': u'1', u'spark.yarn.appMasterEnv.OMP_NUM_THREADS': u'1'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>125</td><td>application_1496667752051_0004</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-cdstod.rw4wupn0px3ehbzqhpqmif0whh.gx.internal.cloudapp.net:8088/proxy/application_1496667752051_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.13:30060/node/containerlogs/container_e07_1496667752051_0004_01_000001/livy\">Link</a></td><td></td></tr></table>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "First, we load the BigDL jar from the Azure Blob Storage account. **Please, make sure you adapted it to your version of Spark! We're using Spark 2.0 in this notebook.**\n\nSecond, we specify additional environment variables required for the BigDL to work:\n* **DL_ENGINE_TYPE** - setting this to use MKL Blas.\n* **MKL_DISABLE_FAST_MM** -  turns the Intel MKL Memory Allocator off for Intel MKL functions to directly use the system malloc/free functions. Intel MKL Memory Allocator uses per-thread memory pools where buffers may be collected for fast reuse.\n* **KMP_BLOCKTIME** - sets the time, in milliseconds, that a thread should wait, after completing the execution of a parallel region, before sleeping.\n* **OMP_WAIT_POLICY** - provides hints about the preferred behavior of waiting threads during program execution; setting it to `\"passive\"` makes threads passive i.e. the threads do not consume processor cycles while waiting\n* **OMP_NUM_THREADS** - sets the number of threads to use for parallel regions\n\nSpecifying the `spark.executorEnv.<environment variable>` sets the environment variables on the executors, whereas setting the `spark.yarn.appMasterEnv.<environment variable>` creates the variables on the head (driver) node; the latter is necessarily set this way as Jupyter runs in a yarn-cluster mode: check http://spark.apache.org/docs/latest/submitting-applications.html#launching-applications-with-spark-submit for more details about the differences between the `client` and `cluster` modes.\n\n---------------", "cell_type": "markdown", "metadata": {}}, {"source": "# Initializing the BigDL Engine\nNext, we import the BigDL engine and initialize it.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "import com.intel.analytics.bigdl.utils.Engine\n\nval nodeNumber = 2\nval coreNumber = 8\nval mult = 64\nval batchSize = nodeNumber * coreNumber * mult\n\nEngine.init(nodeNumber, coreNumber, true)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>126</td><td>application_1496667752051_0005</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-cdstod.rw4wupn0px3ehbzqhpqmif0whh.gx.internal.cloudapp.net:8088/proxy/application_1496667752051_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.14:30060/node/containerlogs/container_e07_1496667752051_0005_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\nwarning: there was one deprecation warning; re-run with -deprecation for details\nres3: Option[org.apache.spark.SparkConf] = Some(org.apache.spark.SparkConf@5a0b46d1)"}], "metadata": {"collapsed": false}}, {"source": "We will use two nodes with eight cores each. The `batchSize` will be later used to split the data into a set of mini batches. Running the `Engine.init(...)` sets several parameters for the BigDL engine to work; each executor will run a multi-threaded operation doing the processing of the data.", "cell_type": "markdown", "metadata": {}}, {"source": "# Creating the model\n\nNow we can prepare our model", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "import com.intel.analytics.bigdl._\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nimport com.intel.analytics.bigdl.nn._\n\ndef buildModel(classNum: Int): Module[Float] = {\n    val model = Sequential()\n    model\n      .add(Reshape(Array(1, 28, 28)))\n      .add(SpatialConvolution(1, 6, 5, 5).setName(\"conv1_5x5\"))\n      .add(Tanh())\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Tanh())\n      .add(SpatialConvolution(6, 12, 5, 5).setName(\"conv2_5x5\"))\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Reshape(Array(12 * 4 * 4)))\n      .add(Linear(12 * 4 * 4, 100).setName(\"fc1\"))\n      .add(Tanh())\n      .add(Linear(100, classNum).setName(\"fc2\"))\n      .add(LogSoftMax())\n  }", "outputs": [{"output_type": "stream", "name": "stdout", "text": "buildModel: (classNum: Int)com.intel.analytics.bigdl.Module[Float]"}], "metadata": {"collapsed": false}}, {"source": "Overall, there are 7 layers in our network: \n\n![](https://raw.githubusercontent.com/ZZUTK/An-Example-of-CNN-on-MNIST-dataset/master/figs/CNN.png)\nsource: https://raw.githubusercontent.com/ZZUTK/An-Example-of-CNN-on-MNIST-dataset/master/figs/CNN.png\n\n1. Input layer\n2. Convolution layer 1 with kernel 5x5\n3. Pooling layer 1\n4. Convolution layer 2 with kernel 5x5\n5. Pooling layer 2\n6. Linearizing layer\n7. Output layer\n\nLet's see how these are implemented by analyzing the code above:\n\n1. The *Input layer* is specified by the `Reshape(...)` method: it takes an image as a input and reshapes the input image to 28x28 matrix (in case it was of a different size).\n2. The *Convolution layer 1 with kernel 5x5* is implemented as a `SpatialConvolution(...)`. The first parameter is the number of input dimensions (in our case 1 as we process grey images, we'd set it to 3 if we used RGB colors). We will output 6 planes that measure 24x24 (Why? Tip: 5x5 and the 28x28 size of the input.) What we are training in this part of the network is the kernel structure. The last two parameters specify size of the kernel (note: the kernels should have odd-numbered dimensions e.g. 3x3 or 7x7 -- why?). The outputs from the kernels are then *squashed* using the `Tanh()` function so the values are in the $<-1; 1>$ range.\n3. The *Pooling layer 1* is added using the `SpatialMaxPooling(...)` method. The pooling *scans* through the outputs of the convolutional layer 1 and averages the output given the kernel size. The first two parameters specify the kernel width and height, the remaining two specify the horizontal and vertical step size. The outputs from this layer are again squashed using the `Tanh()` function.\n4. The *Convolution layer 2 with kernel 5x5* is implemented in a much similar fashion: we again use the 5x5 kernel, we take the 6 outputs from the *Pooling layer 1* and output 12 matrices, each 8x8 in size. \n5. The *Pooling layer 2* brings the outputs from the preceding stage down to 4x4 in size as we, again, use the 2x2 kernels.\n6. The *Linearizing layer* starts with reshaping the output of the final pooling layer into a 1-dimensional array of 192 elements $(12*4*4)$. Next, we add a fully-connected `Linear(...)` layer that uses a linear function $y=Wx+b$ ($W$ stands for the matrix of weights and $b$ is bias). The layer takes the 192 element output from the pooling layer and outputs a 100 values that are being once again squashed by the `Tanh()` function. The last linear layer reduces the number of outputs to 10 - the number of the digits we try to recognize. \n7. Finally, the `LogSoftMax()` layer assigns the digit probability to the input by using the following formula $$y_i=\\log\\Bigg(\\frac{\\exp(x_{i})}{\\sum_{j}\\exp(x_{j})}\\Bigg)$$\n\n-------------------", "cell_type": "markdown", "metadata": {}}, {"source": "# Loading the data\n\nThe format the data is provided in is not really readable by any image program as each file is essentially a stream of bytes. These are not that hard to read it though.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "import java.nio.ByteBuffer\nimport java.nio.file.Path\nimport com.intel.analytics.bigdl.dataset._\nimport com.intel.analytics.bigdl.dataset.image._\nimport com.intel.analytics.bigdl.models.lenet.Utils\n\ndef loadBinaryFile(filePath: Path): Array[Byte] = {\n    val files = sc.binaryFiles(filePath.toString())\n    val bytes = files.first()._2.toArray()\n    bytes\n}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "loadBinaryFile: (filePath: java.nio.file.Path)Array[Byte]"}], "metadata": {"collapsed": false}}, {"source": "The `LoadBinaryFile(...)` takes `filePath` as input and returns an array of bytes. Since the files are stream of binary data we treat them as such when reading them in into Spark.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "def load(featureFile: Path, labelFile: Path): Array[ByteRecord] = {\n    // read the data in\n    val labelBuffer = ByteBuffer.wrap(loadBinaryFile(labelFile))\n    val featureBuffer = ByteBuffer.wrap(loadBinaryFile(featureFile))\n    \n    // check the magic numbers\n    val labelMagicNumber = labelBuffer.getInt()\n    require(labelMagicNumber == 2049)\n    \n    val featureMagicNumber = featureBuffer.getInt()\n    require(featureMagicNumber == 2051)\n    \n    // check if the counts agree between images and labels files\n    val labelCount = labelBuffer.getInt()\n    val featureCount = featureBuffer.getInt()\n    require(labelCount == featureCount)\n\n    // get number of columns and rows per image\n    val rowNum = featureBuffer.getInt()\n    val colNum = featureBuffer.getInt()\n\n    // output buffer\n    val result = new Array[ByteRecord](featureCount)\n  \n    var i = 0\n    while (i < featureCount) {\n        // new image\n        val img = new Array[Byte]((rowNum * colNum))\n        \n        var y = 0\n        while (y < rowNum) {\n            var x = 0\n            while (x < colNum) {\n                // put the read pixel information in the right position\n                img(x + y * colNum) = featureBuffer.get()\n                x += 1\n            }\n            y += 1\n        }\n        \n        // append to results as ByteRecord (add label)\n        result(i) = ByteRecord(img, labelBuffer.get().toFloat + 1.0f)\n        i += 1\n  }\n  result\n}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "load: (featureFile: java.nio.file.Path, labelFile: java.nio.file.Path)Array[com.intel.analytics.bigdl.dataset.ByteRecord]"}], "metadata": {"collapsed": false}}, {"source": "The `load(...)` method requires two parameters: the paths to `featureFile` and `labelFile`; it returns an array of `ByteRecord`s. The `ByteRecord` is a wrapper around an `Array` of bytes with an added label https://github.com/intel-analytics/BigDL/blob/master/spark/dl/src/main/scala/com/intel/analytics/bigdl/dataset/Types.scala.\n\nThe format of the datasets can be found on http://yann.lecun.com/exdb/mnist/ but here it is in a nutshell:\n<pre>\nLABEL FILES:\n\n[offset] [type]          [value]          [description] \n0000     32 bit integer  0x00000801(2049) magic number (MSB first) \n0004     32 bit integer  60000            number of items \n0008     unsigned byte   ??               label \n0009     unsigned byte   ??               label \n........ \nxxxx     unsigned byte   ??               label\nThe labels values are 0 to 9.\n\nTRAINING FILES:\n\n[offset] [type]          [value]          [description] \n0000     32 bit integer  0x00000803(2051) magic number \n0004     32 bit integer  60000            number of images \n0008     32 bit integer  28               number of rows \n0012     32 bit integer  28               number of columns \n0016     unsigned byte   ??               pixel \n0017     unsigned byte   ??               pixel \n........ \nxxxx     unsigned byte   ??               pixel\n</pre>\n\nGiven the above the flow of the function should now be self-explanatory.\n\nNext, we specify the locations of the files.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "val dir = \"/tmp\"\n\nval trainDataFile = \"train-images-idx3-ubyte\"\nval trainLabelFile = \"train-labels-idx1-ubyte\"\nval validationDataFile = \"t10k-images-idx3-ubyte\"\nval validationLabelFile = \"t10k-labels-idx1-ubyte\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "validationLabelFile: String = t10k-labels-idx1-ubyte"}], "metadata": {"collapsed": false}}, {"source": "And read the files.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "import java.nio.file.Paths\nimport com.intel.analytics.bigdl.DataSet\nimport com.intel.analytics.bigdl.dataset._\nimport com.intel.analytics.bigdl.dataset.image._\n\nval trainData = Paths.get(dir, trainDataFile)\nval trainLabel = Paths.get(dir, trainLabelFile)\nval validationData = Paths.get(dir, validationDataFile)\nval validationLabel = Paths.get(dir, validationLabelFile)\n\nval trainMean = 0.13066047740239506\nval trainStd = 0.3081078\nval trainSet = {\n    DataSet.array(load(trainData, trainLabel), sc) -> \n    BytesToGreyImg(28, 28) -> \n    GreyImgNormalizer(trainMean, trainStd) -> \n    GreyImgToBatch(batchSize)\n}\n\nval testMean = 0.13251460696903547\nval testStd = 0.31048024\nval validationSet = {\n    DataSet.array(load(validationData, validationLabel), sc) -> \n    BytesToGreyImg(28, 28) -> \n    GreyImgNormalizer(testMean, testStd) -> \n    GreyImgToBatch(batchSize)\n}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "validationSet: com.intel.analytics.bigdl.dataset.AbstractDataSet[com.intel.analytics.bigdl.dataset.MiniBatch[Float], _] = com.intel.analytics.bigdl.dataset.DistributedDataSet$$anon$5@13fa533b"}], "metadata": {"collapsed": false}}, {"source": "After reading the images using the `load(...)` method, we convert them to grey scale using the `BytesToGreyImg(...)` method. Using the `GreyImgNormalizer(...)` we normalize the image; this changes the range of pixel intensity (also know as contrast or histogram stretching). Lastly, we use the `GreyImgToBatch(...)` method to put the images into batches.\n\n------------------", "cell_type": "markdown", "metadata": {}}, {"source": "# Training the model\n\nNext, we start preparing our model.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "import com.intel.analytics.bigdl.nn._\nimport com.intel.analytics.bigdl.optim._\nimport com.intel.analytics.bigdl.utils._\n\n// Train Lenet model\nval initialModel = buildModel(10)  // 10 digit classes\n\nval optimizer = Optimizer(\n  model = initialModel,                   // training model\n  dataset = trainSet,                     // training dataset\n  criterion = ClassNLLCriterion[Float]()) // loss function", "outputs": [{"output_type": "stream", "name": "stdout", "text": "optimizer: com.intel.analytics.bigdl.optim.Optimizer[Float,com.intel.analytics.bigdl.dataset.MiniBatch[Float]] = com.intel.analytics.bigdl.optim.DistriOptimizer@7c6f52f6"}], "metadata": {"collapsed": false}}, {"source": "The `initialModel` object holds our model definition - we'll attempt to optimize it. The `optimizer` object is the optimization algorithm we'll use to train our network; we pass the `initialModel` as the `model` to be trained and our `trainSet` as the `dataset` to be learned from. The loss function chosen is the negative log-likelihood function - the `ClassNLLCriterion[Float]()`; since the log-likelihood function is concave\n![Example of log-likelihood function](https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/LikelihoodFunctionAfterHHT.png/400px-LikelihoodFunctionAfterHHT.png)\nsource: https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/LikelihoodFunctionAfterHHT.png/400px-LikelihoodFunctionAfterHHT.png\n\nand the optimizer's function is to minimize some cost-function we use the negative log-likelihood function\n![](http://statgen.iop.kcl.ac.uk/media/ml2.gif)\nsource: http://statgen.iop.kcl.ac.uk/media/ml2.gif\n\nCheck http://neuralnetworksanddeeplearning.com/chap3.html if you're interested in learning more about optimization.\n\n--------\n\nNow it's time to optimize the model. We first set the `learningRate` and `maxEpoch` - the two hyperparameters of the network so we control the training rate and when we stop the training. \n\nSetting the validation on the `optimizer` object instructs it to trigger the validation after `eachEpoch` using the `validationSet` (see https://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-networ): after each epoch the `top1Accuracy` is checked to control for the overfitting. We stop the training after 15 epochs.  Finally, we call the `optimize()` method that triggerst the training process to start.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "// Set hyperparameters. we set learningrate and max epochs\nval state = T(\"learningRate\" -> 0.05 / 4 * mult)\n\n// Set maximum epochs\nval maxEpoch = 15\n\nval trainedModel = {optimizer.setValidation(\n        trigger = Trigger.everyEpoch,\n        dataset = validationSet,\n        vMethods = Array(new Top1Accuracy)\n    ).setState(state)\n    .setEndWhen(Trigger.maxEpoch(maxEpoch))\n    .optimize()\n}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "can't find locality partition for partition 0 Partition locations are (ArrayBuffer(10.0.0.14)) Candidate partition locations are\n(0,List())\n(1,List()).\ncan't find locality partition for partition 1 Partition locations are (ArrayBuffer(10.0.0.14)) Candidate partition locations are\n(0,List())\n(1,List()).\ntrainedModel: com.intel.analytics.bigdl.Module[Float] =\nSequential[7f33c097]{\n  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> (12) -> output]\n  (1): Reshape$mcF$sp[d46152f7](1x28x28)\n  (2): SpatialConvolution[conv1_5x5](1 -> 6, 5 x 5, 1, 1, 0, 0)\n  (3): Tanh$mcF$sp[1a67d192]\n  (4): SpatialMaxPooling[e9dcb01e](2, 2, 2, 2, 0, 0)\n  (5): Tanh$mcF$sp[4275fa8e]\n  (6): SpatialConvolution[conv2_5x5](6 -> 12, 5 x 5, 1, 1, 0, 0)\n  (7): SpatialMaxPooling[87b789d2](2, 2, 2, 2, 0, 0)\n  (8): Reshape$mcF$sp[8c1e88e8](192)\n  (9): Linear[fc1](192 -> 100)\n  (10): Tanh$mcF$sp[9abf527f]\n  (11): Linear[fc2](100 -> 10)\n  (12): LogSoftMax[e2ab98e7]\n}"}], "metadata": {"collapsed": false}}, {"source": "Once the training is done (normally it would take ~2-3 minutes) we can check how well we've done.\n\n-------", "cell_type": "markdown", "metadata": {}}, {"source": "# Testing the model\n\nLet's see: using the `Validator` and the `trainedModel` we can test the accuracy of the model on the `validationSet`. Calling the `.test(...)` method on the `validator` will trigger the network to process the validation data and produce the output that is then compared to the original (expected) labels.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "// import com.intel.analytics.bigdl.optim.{LocalValidator, Top1Accuracy, Validator}\n\nval validator = Validator(trainedModel, validationSet)\nval result = validator.test(Array(new Top1Accuracy[Float]))\n\nresult.foreach(r => {\n  println(s\"${r._2} is ${r._1}\")\n})", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Top1Accuracy is Accuracy(correct: 9872, count: 10000, accuracy: 0.9872)"}], "metadata": {"collapsed": false}}, {"source": "As you can we we got a respectable 99% accuracy; if you are willing to let it train for more than 15 epochs the accuracy should increase but be careful not to overfit your model.\n\n------------", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "# Additional resources\n\nNeural networks are well known for solving some of the most complex problems we currently encounter. Here are some interesting resouces for you to browse through:\n\n1. Single-Shot Multibox Object Recognition: https://github.com/intel-analytics/analytics-zoo/blob/master/notebook/example/SSD.ipynb A notebook trained on the MS Coco data to recognize multiple objects from a single image\n2. Automatic Image Captioning: in order to get an appreciation for how complex these structures are (and, on a flip of a coin, how powerful they can get) check this https://cs.stanford.edu/people/karpathy/sfmltalk.pdf", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}